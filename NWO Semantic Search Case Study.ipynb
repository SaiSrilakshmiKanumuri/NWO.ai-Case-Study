{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Semantic Search Algorithm\n",
    "Design and implement a semantic search algorithm that is able to score and rank a\n",
    "set of keywords (trends) by how strongly associated they are to a given query term. A\n",
    "simple approach could borrow techniques from association rule mining to analyze the\n",
    "co-occurrence of terms within a corpora of tweets and reddit posts -- other\n",
    "approaches are also welcome. Regardless of the methodology, the solution should\n",
    "take into consideration the uniqueness of the trend and the recency of the\n",
    "association. For example, the algorithm should be able to determine that the query\n",
    "‘iPhone’ is more strongly associated with trends like ‘MagSafe’, ‘Apple Wallet’, and\n",
    "‘lidar sensor' than it is to “Biden” or “perfume”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "The idea behind semantic search is to embed all entries in the corpus (sentences, paragraphs, or documents) into a vector space. At search time, the query is embedded into the same vector space and the closest embeddings from the corpus are found. These entries should have a high semantic overlap with the query.\n",
    "\n",
    "I am breaking the challenge into below tasks:\n",
    "1. Data Extraction\n",
    "2. Data Cleaning\n",
    "3. Set up Encoder\n",
    "4. Create Index\n",
    "5. Encode and Index\n",
    "6. Search\n",
    "\n",
    "Assumption: Search query - one word; Result - set of similar one-word keywords/trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Extraction\n",
    "To query our Google BigQuery data using Python, we need to connect the Python client to our BigQuery instance. We do so using a cloud client library for the Google BigQuery API. Here I am using google cloud bigquery library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Google Cloud BigQuery library from command line if it is not already installed\n",
    "# pip install --upgrade google-cloud-bigquery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# get credentials from json file - provide path to json file if necessary\n",
    "credentials = service_account.Credentials.from_service_account_file('nwo-sample-5f8915fdc5ec.json')\n",
    "\n",
    "# connect the client to the database\n",
    "project_id = 'nwo-sample'\n",
    "client = bigquery.Client(credentials= credentials,project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the BigQuery client set up and ready to use, we can execute queries on twitter and reddit datasets. For this, we use the query method, which inserts a query job into the BigQuery queue. These queries are then executed asynchronously. As soon as the job is complete, the method returns a Query_Job instance containing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query to get reddit posts dataset\n",
    "reddit_query = \"\"\"\n",
    "    SELECT *\n",
    "   FROM `nwo-sample.graph.reddit`\n",
    "   LIMIT 100\n",
    "\"\"\"\n",
    "#save query results in a dataframe\n",
    "reddit_df = client.query(reddit_query).to_dataframe()\n",
    "\n",
    "# sql query to get twitter tweets dataset\n",
    "tweets_query = \"\"\"\n",
    "    SELECT *\n",
    "   FROM `nwo-sample.graph.tweets`\n",
    "   LIMIT 100\n",
    "\"\"\"\n",
    "#save query results in a dataframe\n",
    "tweets_df = client.query(tweets_query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dcchjis</td>\n",
       "      <td>t3_5n9wdc</td>\n",
       "      <td>t5_37kq2</td>\n",
       "      <td>SkincareAddicts</td>\n",
       "      <td>shewhoentangles</td>\n",
       "      <td>t2_kk75y</td>\n",
       "      <td>Unfortunately *most* dermatologists don't like...</td>\n",
       "      <td>1484253948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd2getq</td>\n",
       "      <td>t3_5pojzy</td>\n",
       "      <td>t5_2qhfj</td>\n",
       "      <td>finance</td>\n",
       "      <td>Hopemonster</td>\n",
       "      <td>t2_6j34a</td>\n",
       "      <td>Varies very widely by the type of strategy and...</td>\n",
       "      <td>1485720418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dcg2ptx</td>\n",
       "      <td>t3_5nk0uu</td>\n",
       "      <td>t5_2uols</td>\n",
       "      <td>Sephora</td>\n",
       "      <td>Mkg823</td>\n",
       "      <td>t2_wjo50</td>\n",
       "      <td>Sorry for the essay!</td>\n",
       "      <td>1484456622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dcst1wk</td>\n",
       "      <td>t3_5ppfsz</td>\n",
       "      <td>t5_2qvj0</td>\n",
       "      <td>immigration</td>\n",
       "      <td>OvernightSiren</td>\n",
       "      <td>t2_7jv36</td>\n",
       "      <td>I sent it on 12/22/16 with the package postmar...</td>\n",
       "      <td>1485187826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dc5y3p4</td>\n",
       "      <td>t3_5msc5y</td>\n",
       "      <td>t5_2qvj0</td>\n",
       "      <td>immigration</td>\n",
       "      <td>deeroorudy</td>\n",
       "      <td>t2_qrux7</td>\n",
       "      <td>to be honest, it is rather difficult to obtain...</td>\n",
       "      <td>1483901414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    link_id subreddit_id        subreddit           author  \\\n",
       "0  dcchjis  t3_5n9wdc     t5_37kq2  SkincareAddicts  shewhoentangles   \n",
       "1  dd2getq  t3_5pojzy     t5_2qhfj          finance      Hopemonster   \n",
       "2  dcg2ptx  t3_5nk0uu     t5_2uols          Sephora           Mkg823   \n",
       "3  dcst1wk  t3_5ppfsz     t5_2qvj0      immigration   OvernightSiren   \n",
       "4  dc5y3p4  t3_5msc5y     t5_2qvj0      immigration       deeroorudy   \n",
       "\n",
       "  author_id                                               body  created_utc  \n",
       "0  t2_kk75y  Unfortunately *most* dermatologists don't like...   1484253948  \n",
       "1  t2_6j34a  Varies very widely by the type of strategy and...   1485720418  \n",
       "2  t2_wjo50                               Sorry for the essay!   1484456622  \n",
       "3  t2_7jv36  I sent it on 12/22/16 with the package postmar...   1485187826  \n",
       "4  t2_qrux7  to be honest, it is rather difficult to obtain...   1483901414  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine first 5 rows of reddit posts data\n",
    "reddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>src_lang</th>\n",
       "      <th>mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>photos</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>link</th>\n",
       "      <th>quote_url</th>\n",
       "      <th>near</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1260022395311542272</td>\n",
       "      <td>1260022395311542272</td>\n",
       "      <td>2020-05-12 01:42:03</td>\n",
       "      <td>UTC</td>\n",
       "      <td>759251</td>\n",
       "      <td>cnn</td>\n",
       "      <td>CNN</td>\n",
       "      <td></td>\n",
       "      <td>“I think what we saw in that exchange with Wei...</td>\n",
       "      <td></td>\n",
       "      <td>[brianstelter]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>596</td>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/CNN/status/126002239531154...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1258471705305067524</td>\n",
       "      <td>1258471705305067524</td>\n",
       "      <td>2020-05-07 19:00:10</td>\n",
       "      <td>UTC</td>\n",
       "      <td>759251</td>\n",
       "      <td>cnn</td>\n",
       "      <td>CNN</td>\n",
       "      <td></td>\n",
       "      <td>The Trump administration will not implement th...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://cnn.it/2T0Xigf]</td>\n",
       "      <td>[]</td>\n",
       "      <td>105</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/CNN/status/125847170530506...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1257379492353409025</td>\n",
       "      <td>1257379492353409025</td>\n",
       "      <td>2020-05-04 18:40:06</td>\n",
       "      <td>UTC</td>\n",
       "      <td>807095</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td></td>\n",
       "      <td>The Chinese authorities are clamping down as g...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://nyti.ms/2WuWVv0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>66</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/nytimes/status/12573794923...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1256221897299234816</td>\n",
       "      <td>1256221897299234816</td>\n",
       "      <td>2020-05-01 14:00:14</td>\n",
       "      <td>UTC</td>\n",
       "      <td>807095</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td></td>\n",
       "      <td>In Opinion\\n\\n\"North Korea’s supreme leader ma...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://nyti.ms/2YztHy3]</td>\n",
       "      <td>[]</td>\n",
       "      <td>40</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/nytimes/status/12562218972...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1259234203880886274</td>\n",
       "      <td>1259234203880886274</td>\n",
       "      <td>2020-05-09 21:30:04</td>\n",
       "      <td>UTC</td>\n",
       "      <td>807095</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td></td>\n",
       "      <td>“He got sick on Tuesday,” Rafael Benjamin's so...</td>\n",
       "      <td></td>\n",
       "      <td>[powellnyt]</td>\n",
       "      <td>[https://nyti.ms/3fB25P2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>28</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/nytimes/status/12592342038...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id      conversation_id           created_at timezone  \\\n",
       "0  1260022395311542272  1260022395311542272  2020-05-12 01:42:03      UTC   \n",
       "1  1258471705305067524  1258471705305067524  2020-05-07 19:00:10      UTC   \n",
       "2  1257379492353409025  1257379492353409025  2020-05-04 18:40:06      UTC   \n",
       "3  1256221897299234816  1256221897299234816  2020-05-01 14:00:14      UTC   \n",
       "4  1259234203880886274  1259234203880886274  2020-05-09 21:30:04      UTC   \n",
       "\n",
       "  user_id username                name place  \\\n",
       "0  759251      cnn                 CNN         \n",
       "1  759251      cnn                 CNN         \n",
       "2  807095  nytimes  The New York Times         \n",
       "3  807095  nytimes  The New York Times         \n",
       "4  807095  nytimes  The New York Times         \n",
       "\n",
       "                                               tweet src_lang        mentions  \\\n",
       "0  “I think what we saw in that exchange with Wei...           [brianstelter]   \n",
       "1  The Trump administration will not implement th...                       []   \n",
       "2  The Chinese authorities are clamping down as g...                       []   \n",
       "3  In Opinion\\n\\n\"North Korea’s supreme leader ma...                       []   \n",
       "4  “He got sick on Tuesday,” Rafael Benjamin's so...              [powellnyt]   \n",
       "\n",
       "                        urls photos  replies_count  retweet_count  \\\n",
       "0                         []     []            596            420   \n",
       "1   [https://cnn.it/2T0Xigf]     []            105            154   \n",
       "2  [https://nyti.ms/2WuWVv0]     []             66            199   \n",
       "3  [https://nyti.ms/2YztHy3]     []             40             70   \n",
       "4  [https://nyti.ms/3fB25P2]     []             28            153   \n",
       "\n",
       "   likes_count hashtags                                               link  \\\n",
       "0          420       []  https://twitter.com/CNN/status/126002239531154...   \n",
       "1          154       []  https://twitter.com/CNN/status/125847170530506...   \n",
       "2          199       []  https://twitter.com/nytimes/status/12573794923...   \n",
       "3           70       []  https://twitter.com/nytimes/status/12562218972...   \n",
       "4          153       []  https://twitter.com/nytimes/status/12592342038...   \n",
       "\n",
       "  quote_url near  \n",
       "0                 \n",
       "1                 \n",
       "2                 \n",
       "3                 \n",
       "4                 "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine first 5 rows of twitter tweets data\n",
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a keyword, our task is to return the most relevant set of keywords. For this analysis, I am only considering tweets and body of reddit posts and storing them in separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_df.tweet\n",
    "reddits = reddit_df.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define method to clean tweets and body of reddit posts\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import html\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def clean(posts):\n",
    "    \n",
    "    for i in range (len(posts)):\n",
    "        \n",
    "        #remove newline “\\n” \n",
    "        x = posts[i].replace(\"\\n\",\"\")\n",
    "        posts[i] = html.unescape(x)\n",
    "        \n",
    "        #remove all special characters and punctuation\n",
    "        posts[i] = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", posts[i])\n",
    "        \n",
    "        #convert to lower case\n",
    "        posts[i] = posts[i].lower()\n",
    "        \n",
    "        #remove picture url\n",
    "        posts[i] = re.sub(r'pictwittercom[\\w]*',\"\", posts[i])\n",
    "        \n",
    "        #remove unicode represted spaces\n",
    "        posts[i] = posts[i].replace('\\xa0', '')\n",
    "        \n",
    "        #remove extra spaces, tabs and line breaks\n",
    "        posts[i] = \" \".join(posts[i].split())\n",
    "        \n",
    "        #remove non alphabetic characters\n",
    "        posts[i] = \" \".join([w for w in posts[i].split() if w.isalpha()])\n",
    "        \n",
    "        #remove short terms (single character)\n",
    "        posts[i] = \" \".join([w for w in posts[i].split() if len(w)>1])\n",
    "        \n",
    "    return posts\n",
    "\n",
    "#clean tweets and reddit posts using the above method\n",
    "cleaned_tweets = clean(tweets)\n",
    "cleaned_reddits = clean(reddits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we already have much cleaner data, but there is one more thing that we need to do to make it even cleaner. In text-data contains insignificant words that are not used for the analysis process because they could mess up the analysis score. So, we’re about to clean them now using the nltk Python library. There are few steps that we need to follow to remove the stopwords: \n",
    "1. Preparing Stop words \n",
    "2. Tokenizing tweets and reddit posts \n",
    "3. Remove stop words from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#To install nltk: pip3 install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preparing stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# we exclude not from the stopwords corpus since removing not from the text will change the context of the text\n",
    "stop_words.remove('not')\n",
    "\n",
    "# define method to tokenize text and remove stop words\n",
    "def tokenize_posts(posts_to_token):\n",
    "    \n",
    "    for i in range(len(posts_to_token)):\n",
    "        posts_to_token[i] = word_tokenize(posts_to_token[i])\n",
    "        \n",
    "    for i in range(len(posts_to_token)):\n",
    "        posts_to_token[i] = [word for word in posts_to_token[i] if not word in stop_words]\n",
    "\n",
    "    return posts_to_token\n",
    "\n",
    "#tokenize tweets and remove stopwords\n",
    "tweets_to_token = cleaned_tweets.copy()\n",
    "tokenized_tweets = tokenize_posts(tweets_to_token)\n",
    "\n",
    "#tokenize reddit posts and remove stopwords\n",
    "reddits_to_token = cleaned_reddits.copy()\n",
    "tokenized_reddits = tokenize_posts(reddits_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    think what we saw in that exchange with weijia...\n",
       "1    the trump administration will not implement th...\n",
       "2    the chinese authorities are clamping down as g...\n",
       "3    in opinionnorth koreas supreme leader may be d...\n",
       "4    he got sick on tuesday rafael benjamins soninl...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets before tokenzing\n",
    "cleaned_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [think, saw, exchange, weijia, jiang, somethin...\n",
       "1    [trump, administration, not, implement, cdcs, ...\n",
       "2    [chinese, authorities, clamping, grieving, rel...\n",
       "3    [opinionnorth, koreas, supreme, leader, may, d...\n",
       "4    [got, sick, tuesday, rafael, benjamins, soninl...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets after tokenzing\n",
    "tokenized_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortunately most dermatologists dont like or...\n",
       "1    varies very widely by the type of strategy and...\n",
       "2                                  sorry for the essay\n",
       "3       sent it on with the package postmarked as such\n",
       "4    to be honest it is rather difficult to obtain ...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reddit posts before tokenzing\n",
    "cleaned_reddits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [unfortunately, dermatologists, dont, like, un...\n",
       "1    [varies, widely, type, strategy, rolesome, pms...\n",
       "2                                       [sorry, essay]\n",
       "3                          [sent, package, postmarked]\n",
       "4    [honest, rather, difficult, obtain, waiver, ea...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reddit posts after tokenzing\n",
    "tokenized_reddits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all tokenized tweets in a list and remove duplicates\n",
    "import itertools\n",
    "all_tweet_words = list(itertools.chain(*tokenized_tweets))\n",
    "unique_tweet_words = list(set(all_tweet_words))\n",
    "\n",
    "#Store all tokenized reddit posts in a list  and remove duplicates\n",
    "all_reddit_words = list(itertools.chain(*tokenized_reddits))\n",
    "unique_reddit_words = list(set(all_reddit_words))\n",
    "\n",
    "#Store tweet tokens and reddit post tokens in a list  and remove duplicates\n",
    "all_words = unique_tweet_words + unique_reddit_words\n",
    "unique_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Set up Encoder\n",
    "Now, let's set up an encoder to encode all the unique words into vectors. SentenceTransformers framework offers several pre-trained models that have been extensively evaluated for their quality to embedded search queries & paragraphs (Performance Semantic Search). Here I am using a pre-trained model (all-MiniLM-L6-v2) as it is 5 times faster than the best quality model and still offers good quality. \n",
    "\n",
    "More details at https://www.sbert.net/examples/applications/semantic-search/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To install SentenceTransformers: pip install -U sentence-transformers\n",
    "\n",
    "#Set up encoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Create Indexer\n",
    "Now we will create FAISS indexer class which will store all embeddings efficiently for fast vector search.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other.\n",
    "\n",
    "More details at:https://ai.facebook.com/tools/faiss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To install faiss: conda install -c pytorch faiss-cpu\n",
    "\n",
    "import faiss\n",
    "\n",
    "class FAISS:\n",
    "    def __init__(self, dimensions:int):\n",
    "        self.dimensions = dimensions\n",
    "        self.index = faiss.IndexFlatL2(dimensions)\n",
    "        self.vectors = {}\n",
    "        self.counter = 0\n",
    "    \n",
    "    def add(self, text:str, v:list):\n",
    "        self.index.add(v)\n",
    "        self.vectors[self.counter] = (text, v)\n",
    "        self.counter += 1\n",
    "        \n",
    "    def search(self, v:list, k:int=10):\n",
    "        distance, item_index = self.index.search(v, k)\n",
    "        for dist, i in zip(distance[0], item_index[0]):\n",
    "            if i==-1:\n",
    "                break\n",
    "            else:\n",
    "                print(f'{self.vectors[i][0]}, %.2f'%dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Encode and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2047/2047 [00:18<00:00, 110.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#Install tqdm: pip3 install tqdm\n",
    "\n",
    "#Create embeddings for all words and store them in FAISS. \n",
    "from tqdm import tqdm\n",
    "\n",
    "dim = model.encode(['hello']).shape[-1]\n",
    "\n",
    "index = FAISS(dim)\n",
    "for q in tqdm(unique_words):\n",
    "    emb = model.encode([q])\n",
    "    index.add(q, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Search\n",
    "Similarity is measured by calculating the euclidean ditance between the search query and embeddings in the corpus. \n",
    "Smaller distance implies stronger the association and vice versa\n",
    "\n",
    "More details at https://faiss.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a search method which shows us the top k similar results (by default 10) given a query.\n",
    "def search(s, k=10):\n",
    "    emb = model.encode([s])\n",
    "    print('keyword  distance')\n",
    "    index.search(emb, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword,  distance\n",
      "coronavirus, 0.00\n",
      "covid, 0.71\n",
      "virus, 0.80\n",
      "viral, 0.83\n",
      "outbreak, 0.99\n",
      "pandemic, 1.08\n",
      "outbreaks, 1.09\n",
      "plague, 1.10\n",
      "vaccine, 1.15\n",
      "antivaccination, 1.24\n"
     ]
    }
   ],
   "source": [
    "search('coronavirus')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
